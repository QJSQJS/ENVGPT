# ENVGPT



<div align="center">
  <a href="https://github.com/shibing624/MedicalGPT">
    <img src="./docs/envgpt.png" height="100" alt="Logo">
  </a>
</div>


-----------------

# ENVGPT: A server environment automatic detection, configuration, and management tool based on a large model.
**ENVGPT** åŸºäºå¤§æ¨¡å‹çš„æœåŠ¡å™¨ç¯å¢ƒè‡ªåŠ¨æ£€æµ‹ã€é…ç½®ä¸ç®¡ç†å·¥å…·ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

<img src="./docs/GPT_Training.jpg" width="860" />

åˆ†å››é˜¶æ®µè®­ç»ƒGPTæ¨¡å‹ï¼Œæ¥è‡ªAndrej Karpathyçš„æ¼”è®²PDF [State of GPT](https://karpathy.ai/stateofgpt.pdf)ï¼Œè§†é¢‘ [Video](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)


## ğŸ˜Š Feature
åŸºäºChatGPT Training Pipelineï¼Œæœ¬é¡¹ç›®å®ç°äº†é¢†åŸŸæ¨¡å‹--ç¯å¢ƒé…ç½®æ¨¡å‹çš„å››é˜¶æ®µè®­ç»ƒï¼š

- ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æ¡£æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥æ³¨å…¥é¢†åŸŸçŸ¥è¯†
- ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾
- ç¬¬ä¸‰é˜¶æ®µï¼šRM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯"HHH"åŸåˆ™ï¼Œå…·ä½“æ˜¯"helpful, honest, harmless"
- ç¬¬å››é˜¶æ®µï¼šRL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬

## ğŸš€ Training Pipeline

Training Stage:

| Stage                           | Introduction |  Python script                                                                                                           | Shell script                                                                        |
|:--------------------------------|:-------------|:------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|
| Stage 1: Continue Pretraining   | å¢é‡é¢„è®­ç»ƒ        |          [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py)                     | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)   |
| Stage 2: Supervised Fine-tuning | æœ‰ç›‘ç£å¾®è°ƒ        | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh) |
| Stage 3: Reward Modeling        | å¥–åŠ±æ¨¡å‹å»ºæ¨¡       | [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py)             | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)   |
| Stage 4: Reinforcement Learning | å¼ºåŒ–å­¦ä¹          |  [rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/rl_training.py)                     | [run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rl.sh)   |

#### Supported Models
The following models are tested:

bloom:
- [bigscience/bloomz-560m](https://huggingface.co/bigscience/bloomz-560m)
- [bigscience/bloomz-1b7](https://huggingface.co/bigscience/bloomz-1b7)
- [bigscience/bloomz-7b1](https://huggingface.co/bigscience/bloomz-7b1)

llama:
- [shibing624/chinese-alpaca-plus-7b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf)
- [shibing624/chinese-alpaca-plus-13b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf)
- [minlik/chinese-llama-plus-7b-merged](https://huggingface.co/minlik/chinese-llama-plus-7b-merged)
- [shibing624/chinese-llama-plus-13b-hf](https://huggingface.co/shibing624/chinese-llama-plus-13b-hf)
- [decapoda-research/llama-7b-hf](https://huggingface.co/decapoda-research/llama-7b-hf)
- [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)

chatglm:
- [THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)

baichuan:
- [baichuan-inc/baichuan-7B](https://huggingface.co/baichuan-inc/baichuan-7B)

## ğŸ’» Inference 


## ğŸ“š Dataset 


æˆ‘ä»¬æ—¢å¯ä»¥åŸºäºGPTï¼Œä¹Ÿå¯ä»¥åŸºäºå…¶ä»–ä»»ä½•é—­æºã€å¼€æºå¤§æ¨¡å‹è¿›è¡Œé›†æˆå¼€å‘ã€‚æˆ‘ä»¬å¯ä»¥è°ƒç”¨APIã€å¾®è°ƒå¤§æ¨¡å‹ï¼Œå¯ä»¥è‡ªå·±ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æ›´é‡è¦çš„æ˜¯æ•°æ®å’Œå®é™…ç”¨æˆ·ï¼šæˆ‘ä»¬æœ‰å¤§é‡å®é™…æ•°æ®ï¼Œæœ‰å„è¡Œä¸šçš„çœŸå®ç”¨æˆ·ï¼Œè¿™æ˜¯æœ€ä¸ºé‡è¦çš„ã€‚

è®ºæ–‡ã€ä¸“åˆ©coming soonï¼

é¡¹ç›®ä¼šå¼€æºï¼Œå¯èƒ½æ¨å‡ºä¼ä¸šç‰ˆã€‚

å›¢é˜Ÿï¼šèˆªå¤©ã€åŒ—äº¬äº¤é€šå¤§å­¦ã€å‰æ—å¤§å­¦ã€ä¸­å›½ç§»åŠ¨
